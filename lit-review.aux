\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Literature review.}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Elements of reinforcement learning}{1}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}The Problem.}{1}{subsection.1.1.1}}
\citation{bertsekas1995dynamic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Agent and environment}{2}{subsection.1.1.2}}
\newlabel{sec:agent-and-environment}{{1.1.2}{2}{Agent and environment}{subsection.1.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The agent-environment cycle (Sutton and Barto, 1998).\relax }}{2}{figure.caption.10}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:agent-env-cycle}{{1.1}{2}{The agent-environment cycle (Sutton and Barto, 1998).\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Agent's goals.}{4}{subsection.1.1.3}}
\citation{bellman1956dynamic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Dynamic Programming influence.}{5}{subsection.1.1.4}}
\@writefile{toc}{\contentsline {subsubsection}{Value Functions and Bellman Equation.}{5}{section*.11}}
\@writefile{toc}{\contentsline {subsubsection}{Optimal Value Functions.}{7}{section*.12}}
\@writefile{toc}{\contentsline {subsubsection}{Value Iteration.}{7}{section*.13}}
\citation{bellman1956dynamic}
\citation{bellman1956dynamic}
\@writefile{toc}{\contentsline {subsubsection}{Policy Iteration.}{8}{section*.14}}
\citation{bellman1956dynamic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.5}Q-Learning.}{9}{subsection.1.1.5}}
\newlabel{sec:q-learning}{{1.1.5}{9}{Q-Learning}{subsection.1.1.5}{}}
\citation{watkins1992q}
\citation{Sutton90time-derivativemodels}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Q-Learning\relax }}{10}{algorithm.1}}
\newlabel{alg:q-learning}{{1}{10}{Q-Learning\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.6}Off-policy and On-policy algorithms.}{11}{subsection.1.1.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.7}SARSA.}{12}{subsection.1.1.7}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Multi-objective Optimization.}{13}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Multi-objective Problem.}{13}{subsection.1.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Tradeoffs.}{13}{subsection.1.2.2}}
\citation{pareto1896cours}
\citation{pareto1896cours}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Dominance.}{14}{subsection.1.2.3}}
\newlabel{sec:dominance}{{1.2.3}{14}{Dominance}{subsection.1.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}The Pareto Front.}{14}{subsection.1.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces The Pareto front is represented with black points. Each black point is a non-dominated solution. Grey points represent dominated solutions. Each gray point is dominated by at least one black point.\relax }}{15}{figure.caption.15}}
\newlabel{fig:paretoFront}{{1.2}{15}{The Pareto front is represented with black points. Each black point is a non-dominated solution. Grey points represent dominated solutions. Each gray point is dominated by at least one black point.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Selecting a Solution in The Pareto Front.}{15}{subsection.1.2.5}}
\newlabel{sec:Selecting-a-solution-in-the-Pareto-Front}{{1.2.5}{15}{Selecting a Solution in The Pareto Front}{subsection.1.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Linear-Weighted Averages.}{16}{section*.16}}
\citation{roijers2013survey}
\citation{roijers2013survey}
\@writefile{toc}{\contentsline {subsubsection}{Lexicographical Ordering.}{17}{section*.17}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Multi-objective Reinforcement Learning Research.}{17}{section.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Multi-Objective Markov Decission Processes (MOMDP).}{18}{subsection.1.3.1}}
\citation{Feinberg93constrainedmarkov}
\citation{Karlsson97learningto}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Linear Temporal Difference Learning.}{19}{subsection.1.3.2}}
\citation{castelletti2002reinforcement}
\citation{Karlsson97learningto}
\citation{natarajan2005dynamic}
\citation{gabor1998multi}
\citation{Yaman20111290}
\citation{gabor1998multi}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Non-linear Temporal Difference Learning.}{22}{subsection.1.3.3}}
\citation{van2012scalarized}
\citation{van2013hypervolume}
\citation{roijers2013survey}
\citation{vamplew2011empirical}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Hypervolume based action-selection\relax }}{25}{algorithm.2}}
\newlabel{alg:hb-as}{{2}{25}{Hypervolume based action-selection\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Simultaneous Learning of More Than One Policy.}{25}{subsection.1.3.4}}
\citation{barrett2008learning}
\citation{lizotte2010efficient}
\citation{barrett2008learning}
\@setckpt{lit-review}{
\setcounter{page}{29}
\setcounter{equation}{0}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{2}
\setcounter{table}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{2}
\setcounter{ALC@line}{11}
\setcounter{ALC@rem}{11}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{Item}{3}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{27}
\setcounter{ContinuedFloat}{0}
\setcounter{thm}{0}
\setcounter{prop}{0}
\setcounter{as}{0}
\setcounter{alg}{0}
\setcounter{section@level}{2}
}
