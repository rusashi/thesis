\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces The agent-environment cycle (Sutton and Barto, 1998).}}{2}{figure.1.1}
\contentsline {figure}{\numberline {1.2}{\ignorespaces The Pareto front is represented with black points. Each black point is a non-dominated solution. Grey points represent dominated solutions. Each gray point is dominated by at least one black point.}}{15}{figure.1.2}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The figure shows how points from a Pareto front approximation and a reference point together form the region of objective space. The black dots are Pareto optimal solutions and the red dot is the reference point.}}{32}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Blue circles represent states; Black arrows represent the actions. In every state left and right actions are available. State transition is deterministic and an action always leads to the same next state. Next to every arrow there is a vector of immediate rewards for choosing this action. The overall return starting from starting state is shown at the bottom of each branch.}}{33}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Blue circles represent states; Black arrows represent the actions. In every state left and right actions are available. State transition is deterministic and an action always leads to the same next state. Next to every arrow there is a vector of immediate rewards for choosing this action. The overall return starting from starting state is shown at the bottom of each branch.}}{34}{figure.2.3}
\contentsline {figure}{\numberline {2.4}{\ignorespaces The blue dot which represents the policy with return [60,60] is marked with red circle around. The single policy algorithm mentioned above will learn the Q-function for the policy associated with the marked blue dot.}}{35}{figure.2.4}
\contentsline {figure}{\numberline {2.5}{\ignorespaces The RL-Glue standard.}}{37}{figure.2.5}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Example of using the hypervolume metric to evaluate two algorithms.}}{40}{figure.2.6}
\contentsline {figure}{\numberline {2.7}{\ignorespaces Example of calculating the hypervolume metric at specified time intervals to keep track of the progress of the algorithms.}}{41}{figure.2.7}
\contentsline {figure}{\numberline {2.8}{\ignorespaces Grid world for Deep Sea Treasure problem.}}{43}{figure.2.8}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Pareto front for Deep Sea Treasure problem.}}{44}{figure.2.9}
\contentsline {figure}{\numberline {2.10}{\ignorespaces Grid world for the Puddleworld problem.}}{45}{figure.2.10}
\contentsline {figure}{\numberline {2.11}{\ignorespaces Pareto front for the Puddleworld problem.}}{46}{figure.2.11}
\contentsline {figure}{\numberline {2.12}{\ignorespaces Grid world for the Resource Gathering problem.}}{46}{figure.2.12}
\contentsline {figure}{\numberline {2.13}{\ignorespaces Optimal Policies for the Resource Gathering problem.}}{46}{figure.2.13}
