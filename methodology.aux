\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{whiteson2011introduction}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Empirical Evaluation.}{28}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction.}{28}{section.2.1}}
\citation{vamplew2011empirical}
\citation{vamplew2008limitations}
\citation{F6363312}
\citation{aissani2008use}
\citation{shabani2009incorporating}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The Empirical Evaluation Methodology.}{30}{section.2.2}}
\citation{vamplew2011empirical}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Performance metrics.}{31}{subsection.2.2.1}}
\citation{berry2008phd}
\citation{zitzler1999pareto}
\@writefile{toc}{\contentsline {subsubsection}{Hypervolume metric.}{32}{section*.18}}
\citation{castelletti2002reinforcement}
\citation{gabor1998multi}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The figure shows how points from a Pareto front approximation and a reference point together define the region of objective space. The black dots are Pareto optimal solutions and the red dot is the reference point. The hyperolume of the enclosed region can be used as a measure of the quality of the frontal approximation.\relax }}{33}{figure.caption.19}}
\newlabel{fig:hyperVolume}{{2.1}{33}{The figure shows how points from a Pareto front approximation and a reference point together define the region of objective space. The black dots are Pareto optimal solutions and the red dot is the reference point. The hyperolume of the enclosed region can be used as a measure of the quality of the frontal approximation.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Experiment Structure.}{33}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{Single-Policy Algorithms.}{33}{section*.20}}
\citation{castelletti2002reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Blue circles represent states; Black arrows represent the actions. In every state left and right actions are available. State transition is deterministic and an action always leads to the same next state. Next to every arrow there is a vector of immediate rewards for choosing this action. The overall return starting from starting state is shown at the bottom of each branch.\relax }}{34}{figure.caption.21}}
\newlabel{fig:decisionTree}{{2.2}{34}{Blue circles represent states; Black arrows represent the actions. In every state left and right actions are available. State transition is deterministic and an action always leads to the same next state. Next to every arrow there is a vector of immediate rewards for choosing this action. The overall return starting from starting state is shown at the bottom of each branch.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Each blue dot is a unique policy from the example MDP problem. \textbf  {NOTE:} Because the example MDP does not have dominated policies all four available policies together constitute the Pareto front for this problem.\relax }}{35}{figure.caption.22}}
\newlabel{fig:exampleMDPFront}{{2.3}{35}{Each blue dot is a unique policy from the example MDP problem. \textbf {NOTE:} Because the example MDP does not have dominated policies all four available policies together constitute the Pareto front for this problem.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The blue dot which represents the policy with return [60,60] is marked with red circle around. The single policy algorithm mentioned above will learn the Q-function for the policy associated with the marked blue dot.\relax }}{36}{figure.caption.23}}
\newlabel{fig:exampleMDPOneMarkedPolicy}{{2.4}{36}{The blue dot which represents the policy with return [60,60] is marked with red circle around. The single policy algorithm mentioned above will learn the Q-function for the policy associated with the marked blue dot.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{Multi-Policy Algorithms.}{36}{section*.24}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces An example state-transition. At time step $t$ the algorithm chose action $a_{t}\tmspace  +\thickmuskip {.2777em}=\tmspace  +\thickmuskip {.2777em}left$ according to the active policy $\pi ^{\mathaccentV {vec}17E{w}^{1}}$. This resulted in transition to a state $s_{t+1}$ and immediate reward [20,20] was received by the algorithm. Even though policy $\pi ^{\mathaccentV {vec}17E{w}^{2}}$ was not involved into action-selection it is still possible to update it's value of $Q(s_{t},a_{t})$.\relax }}{38}{figure.caption.25}}
\newlabel{fig:reasonForMPLearning}{{2.5}{38}{An example state-transition. At time step $t$ the algorithm chose action $a_{t}\;=\;left$ according to the active policy $\pi ^{\vec {w}^{1}}$. This resulted in transition to a state $s_{t+1}$ and immediate reward [20,20] was received by the algorithm. Even though policy $\pi ^{\vec {w}^{2}}$ was not involved into action-selection it is still possible to update it's value of $Q(s_{t},a_{t})$.\relax }{figure.caption.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces \textbf  {Left-hand side table} is the Q-function for the state $s_{t+1}$ for the policy $ \pi ^{\mathaccentV {vec}17E{w}^{1}} $. \textbf  {Right-hand side table} is the Q-function for the state $s_{t+1}$ for the policy $ \pi ^{\mathaccentV {vec}17E{w}^{2}} $.\relax }}{38}{table.caption.26}}
\newlabel{table:Qfunctionsfortplusone}{{2.1}{38}{\textbf {Left-hand side table} is the Q-function for the state $s_{t+1}$ for the policy $ \pi ^{\vec {w}^{1}} $. \textbf {Right-hand side table} is the Q-function for the state $s_{t+1}$ for the policy $ \pi ^{\vec {w}^{2}} $.\relax }{table.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces \textbf  {Left-hand side table} is the Q-function for the state $s_{t}$ for the policy $ \pi _{\mathaccentV {vec}17E{w}_{1}} $. \textbf  {Right-hand side table} is the Q-function for the state $s_{t}$ for the policy $ \pi _{\mathaccentV {vec}17E{w}_{2}} $. Action $right$ was omitted from both Q-functions because it did not play any role in transition from state $s_{t}$ to state $s_{t+1}$.\relax }}{39}{table.caption.27}}
\newlabel{table:updatedQfunctionsfortplusone}{{2.2}{39}{\textbf {Left-hand side table} is the Q-function for the state $s_{t}$ for the policy $ \pi _{\vec {w}_{1}} $. \textbf {Right-hand side table} is the Q-function for the state $s_{t}$ for the policy $ \pi _{\vec {w}_{2}} $. Action $right$ was omitted from both Q-functions because it did not play any role in transition from state $s_{t}$ to state $s_{t+1}$.\relax }{table.caption.27}{}}
\citation{tanner2009rl}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Benchmarking Software.}{40}{subsection.2.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces This diagram depicts the interaction between all modules. As can be seen from the figure all modules are connected to a central RL-Glue server which in turn facilitate the exchange of information between all modules. This figure is from original Tanner and White (2009) RL-Glue publication.\relax }}{42}{figure.caption.28}}
\newlabel{fig:rlglue}{{2.6}{42}{This diagram depicts the interaction between all modules. As can be seen from the figure all modules are connected to a central RL-Glue server which in turn facilitate the exchange of information between all modules. This figure is from original Tanner and White (2009) RL-Glue publication.\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Benchmarks.}{43}{section.2.3}}
\citation{vamplew2011empirical}
\citation{vamplew2011empirical}
\citation{vamplew2011empirical}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Deep sea treasure.}{44}{subsection.2.3.1}}
\citation{boyan1995generalization}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Grid world for Deep Sea Treasure problem. This Figure comes from Vamplew et al (2011).\relax }}{45}{figure.caption.29}}
\newlabel{fig:DSTOverview}{{2.7}{45}{Grid world for Deep Sea Treasure problem. This Figure comes from Vamplew et al (2011).\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}MO-Puddleworld.}{45}{subsection.2.3.2}}
\citation{sutton1996generalization}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Pareto front for Deep Sea Treasure problem. This Figure comes from Vamplew et al (2011).\relax }}{46}{figure.caption.30}}
\newlabel{fig:DSTFront}{{2.8}{46}{Pareto front for Deep Sea Treasure problem. This Figure comes from Vamplew et al (2011).\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}MO-Mountain Car.}{46}{subsection.2.3.3}}
\citation{barrett2008learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Grid world for the Puddleworld problem. This Figure comes from Vamplew et al (2011)\relax }}{47}{figure.caption.31}}
\newlabel{fig:PWOverview}{{2.9}{47}{Grid world for the Puddleworld problem. This Figure comes from Vamplew et al (2011)\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Resource gathering.}{47}{subsection.2.3.4}}
\citation{barrett2008learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Grid world for the Resource Gathering problem. The Figure is taken from Barrett and Narayanan (2008).\relax }}{48}{figure.caption.32}}
\newlabel{fig:RGOverview}{{2.10}{48}{Grid world for the Resource Gathering problem. The Figure is taken from Barrett and Narayanan (2008).\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Optimal Policies for the Resource Gathering problem. The left hand side picture shows all non-dominated policies in the problem's state-space, while the right hand side shows the same policies in the objective space. The Figure is taken from Barrett and Narayanan (2008).\relax }}{48}{figure.caption.33}}
\newlabel{fig:RGAlloptimalPolicies}{{2.11}{48}{Optimal Policies for the Resource Gathering problem. The left hand side picture shows all non-dominated policies in the problem's state-space, while the right hand side shows the same policies in the objective space. The Figure is taken from Barrett and Narayanan (2008).\relax }{figure.caption.33}{}}
\@setckpt{methodology}{
\setcounter{page}{49}
\setcounter{equation}{0}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{4}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{11}
\setcounter{table}{2}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{2}
\setcounter{ALC@line}{11}
\setcounter{ALC@rem}{11}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{Item}{11}
\setcounter{Hfootnote}{5}
\setcounter{bookmark@seq@number}{38}
\setcounter{ContinuedFloat}{0}
\setcounter{thm}{0}
\setcounter{prop}{0}
\setcounter{as}{0}
\setcounter{alg}{0}
\setcounter{section@level}{2}
}
