\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Empirical Evaluation.}{27}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction.}{27}{section.2.1}}
\citation{vamplew2008limitations}
\citation{F6363312}
\citation{aissani2008use}
\citation{shabani2009incorporating}
\citation{vamplew2011empirical}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The Details of the Empirical Evaluation Study.}{29}{section.2.2}}
\citation{berry2008phd}
\citation{zitzler1999pareto}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Performance metrics.}{30}{subsection.2.2.1}}
\citation{castelletti2002reinforcement}
\citation{gabor1998multi}
\@writefile{toc}{\contentsline {subsubsection}{Hypervolume metric.}{31}{section*.18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The figure shows how points from a Pareto front approximation and a reference point together form the region of objective space. The black dots are Pareto optimal solutions and the red dot is the reference point.\relax }}{31}{figure.caption.19}}
\newlabel{fig:hyperVolume}{{2.1}{31}{The figure shows how points from a Pareto front approximation and a reference point together form the region of objective space. The black dots are Pareto optimal solutions and the red dot is the reference point.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Experiment Structure.}{32}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{Single-Policy Algorithms.}{32}{section*.20}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Blue circles represent states; Black arrows represent the actions. In every state left and right actions are available. State transition is deterministic and an action always leads to the same next state. Next to every arrow there is a vector of immediate rewards for choosing this action. The overall return starting from starting state is shown at the bottom of each branch.\relax }}{32}{figure.caption.21}}
\newlabel{fig:decisionTree}{{2.2}{32}{Blue circles represent states; Black arrows represent the actions. In every state left and right actions are available. State transition is deterministic and an action always leads to the same next state. Next to every arrow there is a vector of immediate rewards for choosing this action. The overall return starting from starting state is shown at the bottom of each branch.\relax }{figure.caption.21}{}}
\citation{castelletti2002reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Each blue dot is a unique policy from the example MDP problem. \textbf  {NOTE:} Because the example MDP does not have dominated policies all four available policies together constitute the Pareto front for this problem.\relax }}{33}{figure.caption.22}}
\newlabel{fig:exampleMDPFront}{{2.3}{33}{Each blue dot is a unique policy from the example MDP problem. \textbf {NOTE:} Because the example MDP does not have dominated policies all four available policies together constitute the Pareto front for this problem.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The blue dot which represents the policy with return [60,60] is marked with red circle around. The single policy algorithm mentioned above will learn the Q-function for the policy associated with the marked blue dot.\relax }}{34}{figure.caption.23}}
\newlabel{fig:exampleMDPOneMarkedPolicy}{{2.4}{34}{The blue dot which represents the policy with return [60,60] is marked with red circle around. The single policy algorithm mentioned above will learn the Q-function for the policy associated with the marked blue dot.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{Multi-Policy Algorithms.}{35}{section*.24}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The blue dot which represents the policy with return [60,60] is marked with red circle around. The single policy algorithm mentioned above will learn the Q-function for the policy associated with the marked blue dot.\relax }}{36}{figure.caption.25}}
\newlabel{fig:exampleMDPOneMarkedPolicy}{{2.5}{36}{The blue dot which represents the policy with return [60,60] is marked with red circle around. The single policy algorithm mentioned above will learn the Q-function for the policy associated with the marked blue dot.\relax }{figure.caption.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces \textbf  {Left-hand side table} is the Q-function for the state $s_{t+1}$ for the first set of preferences. \textbf  {Right-hand side table} is the Q-function for the state $s_{t+1}$ for the second set of preferences.\relax }}{36}{table.caption.26}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces \textbf  {Left-hand side table} is the Q-function for the state $s_{t+1}$ for the first set of preferences. \textbf  {Right-hand side table} is the Q-function for the state $s_{t+1}$ for the second set of preferences.\relax }}{36}{table.caption.27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Benchmarking Software.}{36}{subsection.2.2.3}}
\citation{tanner2009rl}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The RL-Glue standard.\relax }}{38}{figure.caption.28}}
\newlabel{rlglue}{{2.6}{38}{The RL-Glue standard.\relax }{figure.caption.28}{}}
\citation{VamplewDBID2011}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Benchmarks.}{40}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Deep sea treasure.}{41}{subsection.2.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Grid world for Deep Sea Treasure problem.\relax }}{41}{figure.caption.29}}
\newlabel{ParetoDominance}{{2.7}{41}{Grid world for Deep Sea Treasure problem.\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}MO-Puddleworld.}{41}{subsection.2.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Pareto front for Deep Sea Treasure problem.\relax }}{42}{figure.caption.30}}
\newlabel{ParetoDominance}{{2.8}{42}{Pareto front for Deep Sea Treasure problem.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Grid world for the Puddleworld problem.\relax }}{42}{figure.caption.31}}
\newlabel{ParetoDominance}{{2.9}{42}{Grid world for the Puddleworld problem.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Pareto front for the Puddleworld problem.\relax }}{43}{figure.caption.32}}
\newlabel{ParetoDominance}{{2.10}{43}{Pareto front for the Puddleworld problem.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}MO-Mountain Car.}{43}{subsection.2.3.3}}
\citation{BarrettN2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Resource gathering.}{44}{subsection.2.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Grid world for the Resource Gathering problem.\relax }}{44}{figure.caption.33}}
\newlabel{ParetoDominance}{{2.11}{44}{Grid world for the Resource Gathering problem.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Optimal Policies for the Resource Gathering problem.\relax }}{45}{figure.caption.34}}
\newlabel{ParetoDominance}{{2.12}{45}{Optimal Policies for the Resource Gathering problem.\relax }{figure.caption.34}{}}
\@setckpt{methodology}{
\setcounter{page}{46}
\setcounter{equation}{0}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{12}
\setcounter{table}{2}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{2}
\setcounter{ALC@line}{11}
\setcounter{ALC@rem}{11}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{Item}{6}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{37}
\setcounter{ContinuedFloat}{0}
\setcounter{thm}{0}
\setcounter{prop}{0}
\setcounter{as}{0}
\setcounter{alg}{0}
\setcounter{section@level}{2}
}
