\chapter{Empirical Evaluation.}

\section{Introduction.}

Whiteson and Littman (2011)\nocite{whiteson2011introduction} argued the importance of the empirical evaluation of reinforcement learning algorithms due to the natural limitations of other means to assess algorithms:

\begin{itemize}
  \item Subjective assessment (intuitive understanding) of an algorithm is prone to initial erroneous assumptions.
  \item Theoretical assessment (rigorous proof) often results in the mismatch of actual performance in a realistic domain with predicted theoretical performance. This is due to the fact that in realistic domains assumptions (that were used during rigorous proof) do not always hold.
\end{itemize}
Thus the empirical evaluation is an important part of a research process.

To conduct an empirical study one needs means to compare different algorithms. For example, classification algorithms are tested on data sets from special repositories. These data sets guarantee that different algorithms will be tested on the same set of benchmarks and thus they introduce more consistency into an assessment. They also allow an experimenter to make cross-comparison between different algorithms.

Reinforcement learning in general and multi-objective reinforcement learning in particular are very different from research areas like classification or clustering. Reinforcement learning is a multi-step decision making process with a number of actions available at each state at every time step. Add to this a factor of randomness associated with state transition and reward functions. Having all these factors substantially complicates the creation of a test problem. In reinforcement learning a test problem is not a static data set but rather a piece of software which reacts dynamically to the actions of an agent.

The absence of a central repository of test problems for multi-objective reinforcement learning lead to the fact that every published paper (prior to 2011\footnote{Vamplew et all. (2011)\nocite{vamplew2011empirical} introduced a number of test problems which were picked up later by other researchers.}) is disconnected from all other papers in terms of test problems. All papers described in the literature review present a brand new test environments. Moreover the majority of the papers use only a small amount of test problems, usually two\footnote{Peter, I still need some time to clarify how many test problems I will use, after that I will correct this section if needed}. This is in contrast with other areas of machine learning where each published paper is connected with other papers by using the same test data sets and the number of tested data sets is usually high, thereby allowing meaningful comparisons to be made between different algorithms.

The last section of the first chapter reviewed some of the most important research in temporal difference multi-objective reinforcement learning. All reviewed papers were mainly concerned with proving the convergence of their algorithms. In each paper authors presented a small number of their own test problems and their own methodology. It is enough in terms of convergence proof but clearly not enough to provide the full guidance about the types of problems for which the algorithm will perform well. Importantly, the convergence proofs do not generally provide any guarantees about the speed of convergence. The following information may be included in such a guidance:
\begin{itemize}
  \item Information about the structure of a reward function. An extrinsic reward function assigns zero rewards for the majority of state transitions of an environment except for special ones (for example, a transition to a goal state). In contrast, an intrinsic reward function is non-zero most of the time (for example, a time penalty applied at every time step to make an agent learn the shortest path). For more information about the application of both extrinsic and intrinsic reward functions in reinforcement learning refer to Uchibe and Doya (2007)\nocite{Uchibe2007}.
  \item Each test problem must be built around a particular property, which can be observed in realistic domains, such as a shape of the Pareto front.
  \item Each algorithm should be subjected to a range of different test problems with varying characteristics, to identify a realistic domain of application.
\end{itemize}

One can also notice the absence of a common methodology across all reviewed papers. Thus one cannot easily compare the results of an algorithm presented in one paper with the results of a different algorithm. Even if the same test problem is used, each methodology essentially incorporates a unique idea of how to show whether an algorithm is effective or not, and those ideas may drastically differ. Overall it substantially complicates the process of comparing the results presented in different papers. One major example can clarify the point. In multi-objective optimization the quality of an algorithm is evaluated by how close the algorithm approximates the Pareto front of a problem. Different methodologies measure this notion of "closeness" in a different way (Berry, 2008\nocite{berry2008phd}) and therefore it becomes meaningless to compare the results which were obtained using different methodologies.

The arguments above can explain the absence of empirical comparisons so far. No single attempt was made to perform a consolidated empirical evaluation of the proposed algorithms. Some of the papers provided empirical comparisons but only to prior versions of the same algorithms.

\section{The Empirical Evaluation Methodology.}

All of the works reviewed in the literature review chapter have the same limitations:
\begin{itemize}
  \item Limited number of test problems.
  \item No mention about the inability of the linear scalarization based algorithms to find concave points of the Pareto front. This limitation was quite known in traditional multi-objective optimization but until recently (Vamplew et al., 2008\nocite{vamplew2008limitations}) was never specifically mentioned in reinforcement learning theory. As a result later research work with uniform weights (Ferreira et al., 2012\nocite{F6363312}, Aissani, Beldjilali, and Trentesaux (2008)\nocite{aissani2008use}, Shabani (2009)\nocite{shabani2009incorporating}) also did not mention possible limitations of this approach.
  \item Each author or team of authors used their own evaluating methodology which complicates the cross-comparison.
\end{itemize}

This document will use the methodology previously proposed by Vamplew et al. (2011)\nocite{vamplew2011empirical}. This paper outlined very important aspects that should be introduced:
\begin{itemize}
  \item MORL algorithms should be tested on a wider range of test problems, and the properties of these problems should be understood to aid in interpreting the variations in the performance of algorithms.
  \item Standard benchmark problems and standard implementations of these problems should be established so as to facilitate comparison of the performance of different algorithms.
  \item Standard approaches to experimental methodology and reporting of results should be adopted, again to aid in the comparison of algorithms between papers. In particular numeric measures of performance will prove more useful for this purpose than the graphical reporting of results.
\end{itemize}

\subsection{Performance metrics.}
\label{sec:performance-metrics}
Every multi-objective problem implies the existence of a set of Pareto optimal solutions. The task of the learning algorithm is to find an approximation to that front. The quality of the approximation defines the quality of the algorithm.

Although every task has a true set of Pareto optimal solutions, machine learning algorithms can usually only approximate this set, and algorithms may vary in how well they estimate the front. Thus performance metrics should be introduced to facilitate in assessing the quality of the found set of Pareto optimal solutions.  Multi-objective reinforcement learning is similar to general multi-objective optimization in terms of a set of Pareto optimal solutions and so metrics previously considered in multi-objective optimization could easily be used in MORL.

Each approximation to a set of Pareto optimal solutions captures only particular points of the true set. How are these points spread relative to each other? How many points are captured? And to which extent does the the approximation cover the true set? All these important features could be encapsulated into a metric which allows us to compare two different learning algorithms. Early work in multi-objective optimization used scalar representations of these features. But each scalar metric captures only specific detail of the front such as diversity, cardinality or accuracy.

Later work in multi-objective optimization used different approach. Instead of multiple number of scalar metrics, composite metrics compatible with the notion of Pareto dominance were proposed. Berry (2008)\nocite{berry2008phd} presented arguments for composite metrics over a number of scalar ones. A number of such approaches were used in multi-objective optimization. For the purposes of this study, I will follow the lead of Vamplew et al. (2011)\nocite{vamplew2011empirical} and focus on the hypervolume metric.

\subsubsection{Hypervolume metric.}

The hypervolume metric (Zitzler and Thiele, 1999\nocite{zitzler1999pareto}) is a well-recommended composite metric which is used to evaluate the quality of an approximate Pareto front found by a learning algorithm. It measures the volume of the objective space region which is dominated by the points in the found Pareto front approximation. One additional point is also added before the volume is measured. This point is dominated by all points in the Pareto front approximation and serves as a reference point. An example is shown in figure~\ref{fig:hyperVolume}. It is important to note that the reference point must be consistent across different experiments because the value of the hypervolume is directly affected by the choice of the reference point. Even the same approximate Pareto front will give different values of hypervolume if different reference points are used, not to mention the ones produced by different algorithms. Thus it is safe to say that the choice of the reference point must be a part of the definition of the benchmark.

Given two learning algorithms, an experimenter can run them on the same test problem. Each algorithm will produce its approximate Pareto front. The experimenter can then calculate each approximation's hypervolume. The approximation which produces a higher value of the hypervolume is a better approximation to the true Pareto front. Importantly, improvements in any of the scalar measures of quality (diversity, cardinality and accuracy) will all be reflected in an increased value for hypervolume.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.9]{hypervolume.png}
\caption{The figure shows how points from a Pareto front approximation and a reference point together define the region of objective space. The black dots are Pareto optimal solutions and the red dot is the reference point. The hypervolume of the enclosed region can be used as a measure of the quality of the frontal approximation.}
\label{fig:hyperVolume}
\end{figure}

\subsection{Experiment Structure.}
\label{sec:experiment-structure}
\subsubsection{Single-Policy Algorithms.}

Single-policy algorithms (Castelleti et al., 2002\nocite{castelletti2002reinforcement}, Gabor, Kalmar and Szepesvari, 1998\nocite{gabor1998multi}) are initialized with a set of preferences. After that the algorithm executes a number of learning episodes and hopefully it will learn the Q-function of the optimal policy for the specified preferences.

To illustrate the work of single policy algorithms consider the following example of a simple MDP which consists only of three time steps. On every time step two actions are available - left and right. State transition is deterministic i.e. each action always leads to the same successor state. For more details refer to Figure~\ref{fig:decisionTree}.\\

\begin{figure}[ht]
\centering
\includegraphics[scale=0.6]{decisionTree.png}
\caption{Blue circles represent states; Black arrows represent the actions. In every state left and right actions are available. State transition is deterministic and an action always leads to the same next state. Next to every arrow there is a vector of immediate rewards for choosing this action. The overall return starting from starting state is shown at the bottom of each branch.}
\label{fig:decisionTree}
\end{figure}

One can identify four different deterministic policies for this example MDP. Essentially, every branch of this tree defines a unique policy with it's associated return.

It is possible to look at this problem from a different perspective - in terms of the objective space. Figure~\ref{fig:exampleMDPFront} shows the two dimensional objective space for the example MDP problem. Each blue dot corresponds to a unique policy and it's coordinates are exactly this policy's return. Overall there are four points in the objective space, one for each policy. One can notice that this four points together constitute the Pareto front for this problem. This is because the example MDP problem doesn't have dominated policies.\\

\begin{figure}[ht]
\centering
\includegraphics[scale=0.6]{exampleMDPFront.png}
\caption{Each blue dot is a unique policy from the example MDP problem. \textbf{NOTE:} Because the example MDP does not have dominated policies all four available policies together constitute the Pareto front for this problem.}
\label{fig:exampleMDPFront}
\end{figure}

Consider any single-policy algorithm, for example the linear scalarization approach discussed in Castelleti et al. (2002)\nocite{castelletti2002reinforcement}. This algorithm requires a set of weights to be specified before the algorithm can start learning; Let $ \textbf{w} = \{0.4,0.6\} $ be this set of weights. Essentially, this algorithm is the multi-objective reinforcement learning adaptation of the linear-weighted average algorithm from general multi-objective optimization. Linear-weighted average algorithm will find which of the available solutions (policies) will maximize the dot product of the return vector and the weight vector. In case of the example MDP, the policy with the return of [60,60] (see Figure~\ref{fig:exampleMDPOneMarkedPolicy}) will maximize the weighted average. The single-policy algorithm will learn the Q-function associated with this policy.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.6]{exampleMDPOneMarkedPolicy.png}
\caption{The blue dot which represents the policy with return [60,60] is marked with a red circle. The linear-scalarized single policy algorithm mentioned above will learn the Q-function for the policy associated with the marked blue dot, when given weight \{0.4,0.6\}}
\label{fig:exampleMDPOneMarkedPolicy}
\end{figure}

Even though the algorithm was able to identify one element of the Pareto front, it is still preferable to find the whole approximation of the front as an algorithm is only useful if it performs well across it's entire range of parameters. The only way to achieve this with single policy algorithms is to reset the preferences every time the algorithm tries to identify another member of the Pareto front. This requires a special experiment design:

\begin{enumerate}
  \item Initialize an array of preferences, where each preference is an array itself (for example a set of weights).
  \item For each item in the array of preferences run a single-policy algorithm for a specified number of episodes. After the algorithm has converged, extract the greediest Q-value of a starting state and record it. \textbf{NOTE:} Notice that a priori an experimenter cannot identify which member of the Pareto front will be targeted by a certain set of preferences. One can only wait until the algorithm has learned the Q-function of a policy which is associated with the set of preferences. After that the greediest Q-value of the starting state will identify which point in the Pareto front was identified.
  \item Combine all recorded points into an approximation of the Pareto front.
\end{enumerate}

This experiment setting will allow us to create an effective approximation of the Pareto front of an underlying problem. It is vital to have this approximation if one is to compare the performances of different algorithms.

\subsubsection{Multi-Policy Algorithms.}
Single-policy algorithms are initialized with a set of parameters, usually its a set of weights - one for each objective. Eventually, this set of parameters gets mapped into one of the policies and the algorithm will learn the Q-function for this policy. As was mentioned before, an experimenter needs to prepare a number of sets of preferences (weights), and restart the experiment for every set of preferences, hopefully leading to a good approximation of the entire front.

However, it is possible to organize a learning process in a way which will allow us to learn more than one policy at a time. Instead of sequential working with each set of parameters one can work with all sets of parameters at the same time. For example, an algorithm might be given the following set - $ \{\;\{0.7,0.3\}, \{0.3,0.7\}\;\} $. Here the algorithm works with two sets of weights. Each set of weights represents the relative importance of each objective to an experimenter. Ideally, both sets will get mapped into different policies, although they both might get mapped into the same policy.  The algorithm then should learn the Q-functions for those mapped policies.

In this setting, one of the mapped policies is used as an active policy; It's Q-function is used during action-selection period. Other mapped policies are used only for learning purposes. This approach requires a special experiment design:
\begin{enumerate}
  \item Initialize an array of preferences, where each preference is an array itself(for example a set of weights).
  \item Create and initialize a number of Q-functions - one for each set of preferences.
  \item Run a specified number of episodes. In every episode, during every state-transition, use received immediate reward to update every Q-function.
  \item After learning is finished, run a greedy episode for each Q-function (if an environment's state-transition and reward functions are deterministic) or multiple episodes if the environment is stochastic in any way and record the resulting return.
  \item Combine all returns into an approximation of the Pareto front.
\end{enumerate}

However, not all multi-policy algorithms require such parameters (for example, Lizotte et al., 2010, Barrett et al., 2008). As was mentioned in Section \ref{sec:simultaneous-learning-of-more-than-one-policy}, these algorithms learn all optimal policies and use parameters only after a learning phase has finished.

There is a trade-off between two types of multi-policy algorithms:
\begin{itemize}
  \item Parallel learning of policies (each policy specified by a set of parameters) is easier to implement but it requires a somewhat complicated experiment design.
  \item On the other hand, learning all policies without using parameters requires a less complicated experiment structure but the algorithms themselves are much more complicated.
\end{itemize}
%Single-policy algorithms are initialized with a set of parameters, usually its a set of weights - one for each objective. Eventually, this set of parameters gets mapped into one of the policies and the algorithm will learn the Q-function for this policy. As was mentioned before, an experimenter needs to prepare a number of sets of preferences (weights), and hopefully those sets will approximate the Pareto front and restart the experiment for every set of preferences.
%
%This experiment setting is not very efficient - not only it takes more computer time but also significantly complicates an experiment design. Instead of sequential working with each set of parameters one can work with all sets of parameters at the same time. For example, an algorithm might be given the following set - $ \{\;\{0.7,0.3\}, \{0.3,0.7\}\;\} $. Here the algorithm works with two sets of weights. Each set of weights represents the relative importance of each objective to an experimenter. Ideally, both sets will get mapped into different policies, although they both might get mapped into the same policy.  The algorithm then should learn the Q-functions for those mapped policies.
%
%In this setting, one of the mapped policies is used as an active policy; It's Q-function is used during action-selection period. Other mapped policies are used only for learning purposes.
%
%Simultaneous learning of multiple policies is possible because state-transition dynamics provides enough information for more than one policy to be successfully updated. As an example consider state-transition depicted on Figure~\ref{fig:reasonForMPLearning}. Let $ \vec{w}^{1}=\{0.7,0.3\} $  and $ \vec{w}^{2}=\{0.3,0.7\} $ be two weight vectors. Each vector represents relative importance of each objective. Weight vectors $ \vec{w}^{1} $  and $ \vec{w}^{2} $ will get mapped into policies $ \pi^{\vec{w}^{1}} $  and $ \pi^{\vec{w}^{2}} $ respectively. Let both policies be deterministic.
%
%Further let $ \pi^{\vec{w}^{1}} $ be an active policy. At time step $ t $ action-selection was made according to active policy $ \pi^{\vec{w}_{1}} $ and let $ a_{t}=left $. Pair $ (s_{t}, a_{t}) $ makes an environment to transition to state $ s_{t+1} $ and the immediate reward [20,20] is received by a learning algorithm.
%
%\begin{figure}[ht]
%\centering
%\includegraphics[scale=0.6]{reasonForMPLearning.png}
%\caption{An example state-transition. At time step $t$ the algorithm chose action $a_{t}\;=\;left$ according to the active policy $\pi^{\vec{w}^{1}}$. This resulted in transition to a state $s_{t+1}$ and immediate reward [20,20] was received by the algorithm. Even though policy $\pi^{\vec{w}^{2}}$ was not involved into action-selection it is still possible to update it's value of $Q(s_{t},a_{t})$.}
%\label{fig:reasonForMPLearning}
%\end{figure}
%
%At time step $ t+1 $ the algorithm needs to update $ Q(s_{t}, a_{t}) $ for both $ \pi^{\vec{w}_{1}} $  and $ \pi^{\vec{w}_{2}} $. To update $ Q(s_{t}, a_{t}) $ the algorithm needs to identify greedy action for a state $ s_{t+1} $ and use Q-value of that greedy action in combination with the immediate reward. For each policy the greedy action might be different but the immediate reward will be the same. This allows to update Q-function for the policy $ \pi^{\vec{w}^{2}} $ even though the state-transition was generated by the policy $ \pi^{\vec{w}^{1}} $.
%
%Table \ref{table:Qfunctionsfortplusone} shows the Q-functions for the state $ s_{t+1} $ for both policies $ \pi^{\vec{w}^{1}} $  and $ \pi^{\vec{w}^{2}} $. These two Q-functions are identical because both actions lead to the terminal state and only immediate reward is used during the update.
%
%{\renewcommand{\arraystretch}{1.2}
%\begin{table}[!htb]
%    \caption{\textbf{Left-hand side table} is the Q-function for the state $s_{t+1}$ for the policy $ \pi^{\vec{w}^{1}} $. \textbf{Right-hand side table} is the Q-function for the state $s_{t+1}$ for the policy $ \pi^{\vec{w}^{2}} $.}
%    \begin{minipage}{.5\linewidth}
%      \centering
%        \begin{tabular}{c|c|c}
%            state $\backslash$ action & left & right \\
%            \hline
%            $s_{t+1}$ & [60,40] & [40,60]
%        \end{tabular}
%    \end{minipage}%
%    \begin{minipage}{.5\linewidth}
%      \centering
%        \begin{tabular}{c|c|c}
%            state $\backslash$ action & left & right \\
%            \hline
%            $s_{t+1}$ & [60,40] & [40,60]
%        \end{tabular}
%    \end{minipage}
%    \label{table:Qfunctionsfortplusone}
%\end{table}
%
%For each policy $\pi$ the greedy action at time step $t+1$ is an action $a$ which will maximize the weighted sum:
%$$ \displaystyle\sum_{i=1}^{n} w^{\pi}_{i}Q^{\pi}_{i}(s_{t+1},a), $$
%where $n$ is the number of objectives. For the policy $ \pi^{\vec{w}^{1}} $ the weight vector is $ \vec{w}^{1}=\{0.7,0.3\} $ and the greedy action is $ left $ because
%$$ (60 \times 0.7) + (40 \times 0.3) > (40 \times 0.7) + (60 \times 0.3). $$
%Here $ (60 \times 0.7) + (40 \times 0.3) $ is the dot product $ \vec{w}^{1} \cdot Q^{\pi^{\vec{w}^{1}}}(s_{t+1},left) $ and $ (40 \times 0.7) + (60 \times 0.3) $ is the dot product $ \vec{w}^{1} \cdot Q^{\pi^{\vec{w}^{1}}}(s_{t+1},right) $. As you can see the dot product for action $left$ produces a higher value than the dot product for action $right$. The weight vector $\vec{w}^{1}$ has a higher value for the first objective. This will be translated into higher priority of the first objective and taking action $left$ in state $s_{t+1}$ results in higher expected return for the first objective. On the other hand for the policy $ \pi^{\vec{w}^{2}} $ the weight vector is $ \vec{w}^{2}=\{0.3,0.7\} $ and the greedy action is $ right $ because
%
%$$ (60 \times 0.3) + (40 \times 0.7) < (40 \times 0.3) + (60 \times 0.7). $$
%Here $ (60 \times 0.3) + (40 \times 0.7) $ is the dot product $ \vec{w}^{2} \cdot Q^{\pi^{\vec{w}^{2}}}(s_{t+1},left) $ and $ (40 \times 0.3) + (60 \times 0.7) $ is the dot product $ \vec{w}^{2} \cdot Q^{\pi^{\vec{w}^{2}}}(s_{t+1},right) $.
%
%Update rule for $ Q^{ \pi^{\vec{w}^{1}} }(s_{t},a_{t}) $ will take the following form:
%$$ Q^{ \pi^{\vec{w}^{1}} }(s_{t},a_{t}) \leftarrow Q^{ \pi^{\vec{w}^{1}} }(s_{t},a_{t}) + \alpha \; \left[\;r_{t+1} + \gamma\; Q^{ \pi^{\vec{w}^{1}} }(s_{t+1},left) - Q^{ \pi^{\vec{w}^{1}} }(s_{t},a_{t})\;\right]. $$
%And update rule for $ Q^{ \pi^{\vec{w}^{2}} }(s_{t},a_{t}) $ will take the following form:
%$$ Q^{ \pi^{\vec{w}^{2}} }(s_{t},a_{t}) \leftarrow Q^{ \pi^{\vec{w}^{2}} }(s_{t},a_{t}) + \alpha \; \left[\;r_{t+1} + \gamma\; Q^{ \pi^{\vec{w}^{2}} }(s_{t+1},right) - Q^{ \pi^{\vec{w}^{2}} }(s_{t},a_{t})\;\right]. $$
%Table \ref{table:updatedQfunctionsfortplusone} shows updated Q-functions for the state $s_{t}$ for both policies $\pi^{\vec{w}^{1}}$ and $\pi^{\vec{w}^{2}}$.
%
%{\renewcommand{\arraystretch}{1.2}
%\begin{table}[!htb]
%    \caption{\textbf{Left-hand side table} is the Q-function for the state $s_{t}$ for the policy $ \pi_{\vec{w}_{1}} $. \textbf{Right-hand side table} is the Q-function for the state $s_{t}$ for the policy $ \pi_{\vec{w}_{2}} $. Action $right$ was omitted from both Q-functions because it did not play any role in transition from state $s_{t}$ to state $s_{t+1}$.}
%    \begin{minipage}{.5\linewidth}
%      \centering
%        \begin{tabular}{c|c|c}
%            state $\backslash$ action & left & right \\
%            \hline
%            $s_{t}$ & [80,60] & ...
%        \end{tabular}
%    \end{minipage}%
%    \begin{minipage}{.5\linewidth}
%      \centering
%        \begin{tabular}{c|c|c}
%            state $\backslash$ action & left & right \\
%            \hline
%            $s_{t}$ & [60,80] & ...
%        \end{tabular}
%    \end{minipage}
%    \label{table:updatedQfunctionsfortplusone}
%\end{table}

\subsection{Benchmarking Software.}
As was mentioned before a test problem in reinforcement learning is a dynamic piece of software, which actively responds and changes according to the actions from an agent. The agent is also of a dynamic nature which in a similar manner dynamically reacts to the changes in the test problem. This is due to the inherent interactive nature of the the agent-environment cycle. For example every test problem should have the means (a function or a method) to accept an action from the agent. As well as means (again a function or a method) to pass information about state-transition and reward functions to the agent.

Even though test problems may have nothing in common in terms of realistic domains they represent, they still share those means of interaction with an agent or learning algorithm. Moreover in every problem the information that is passed between an agent and an environment is of very similar nature: the agent informs the environment about it's preferred action, the environment notifies the agent about the consequences of that particular action.

For a long time nobody took advantage of that fact; only different programming techniques were used to implement the interaction between the agent and the environment and also different programming languages and different platforms were used. Even if two authors used the same test problem it is possible that differences may exist in their implementations of that problem. This created a natural level of complexity which prevented the exchange of learning algorithms and test problems.

Eventually Tanner and White (2009)\nocite{tanner2009rl} created an open source software called RL-Glue. This software lays a foundation for empirical studies in reinforcement learning because it provides a necessary set of standards which allows researchers to connect the test problems with the learning algorithms from different researchers. The framework handles all the complexities hard work of connecting and passing information between algorithms and problems. For example, experimenters who want to test their newly developed algorithm on a set of well established test benchmarks does not need to think about how their code will interact with different problems; They just encapsulates the logic of their algorithm into standard functions. After that RL-Glue will be able to connect this algorithm to any test problem. Essentially, RL-Glue, by introducing a set of standard functions , allows researchers to plug-in agents, environments and experiments from different authors even created in different languages.

RL-Glue consists of three main modules as shown in Figure \ref{fig:rlglue}: an agent, an environment and an experiment. The agent is the module which encapsulates a learning algorithm. The environment is the module which encapsulates a particular test problem. The experiment module is a place where a researcher controls how many time steps per episode or how many episodes should be performed by the agent. All the interaction between the modules is handled by a central server RL-Glue server.

\begin{figure}[ht]
\vskip 0.2in
\centering
\includegraphics[scale=0.9]{glue.png}
\caption{This diagram depicts the interaction between all modules. As can be seen from the figure all modules are connected to a central RL-Glue server which in turn facilitate the exchange of information between all modules. Tanner and White (2009).}
\vskip -0.2in
\label{fig:rlglue}
\end{figure}
The plug-in nature of the RL-glue framework perfectly addresses three aspects of adequate methodology that were outlined before. Namely:
\begin{itemize}

\item The agent module encapsulates the learning algorithm. Each learning algorithm is a separate plug-in. Multiple numbers of learning algorithms can be attached to the RL-Glue simultaneously. This allows an experimenter to evaluate multiple algorithms simultaneously. A researcher just chooses which of the connected agents will be evaluated.

\item The environment module tackles aspect number two of the adequate methodology. Because the environment module is well-documented and well-standardized in terms of standard programming interfaces it allows the creation and reuse of standard test problems. This module eliminates connecting code needed previously. With plug-in architecture the researcher only needs to download a test problem and plug it into a running RL-Glue server without writing any code to connect the problem and the algorithm.

\item The experiment module tackles aspect number three of the adequate methodology. The experiment module is the plug-in where the researcher chooses the particular test problem and the particular learning algorithms that will be evaluated against that test problem. The experiment module is also a place where all the statistics are gathered and analyzed. This is achieved by introducing a standard message protocol between the agent module and the experiment module. Using this message protocol the agents can send information about learned Q-functions and policies to the experiment module. This is also a place where different performance metrics are applied. Each performance metric is created as a library with external functions which encapsulates the inner routines. Such performance metrics are stored separately in the library files and also could be accessible to the public. For example hypervolume performance metric will be introduced to RL-Glue as a part of this thesis and will be available publicly.

\end{itemize}

Originally, RL-Glue is a software for single objective reinforcement learning but it can be adapted to multi-objective (which is also a part of this thesis) by replacing scalar reward signal with a vector-valued signal.

\subsection{Multi-Objective RL-Glue framework.}
RL-Glue framework is a perfect testing platform for reinforcement learning but it is hardcoded to work with single-objective domains. As a part of the contribution made by this thesis the RL-Glue framework was extended to work with multi-objective domains. RL-Glue framework is an open source software which allowed me to thoroughly study the architecture of the framework and to introduce changes which allowed the RL-Glue framework to represent a reward signal as a vector.

\section{Benchmarks.}

Each benchmark or test problem is created to address a particular aspect of real-life problems. Thus a rich library of benchmarks should be available to adequately assess a learning algorithm. A suite of these benchmarks will allow to look at the learning from different point of views. Combined together, the benchmarks will allow researchers to evaluate how different algorithms cope with the following aspects of real-life problems:

\begin{itemize}

\item Multiple number of objectives. To test which algorithms cope better when the number of objectives is increased.

\item A range of benchmarks should be devoted to assess algorithms under stochasticity of reward and state transition functions.

\item One of the basic requirements is to create benchmarks with continuous state and action spaces.

\item Another important class of benchmarks is ones describing real-life problems with large dimensionality of state or action spaces that require the use of function approximation;

\item Problems with partially-observable states.

\item A mixture of episodic and continuing tasks;

\item Each real-life problem assumes existence of Pareto front of optimal solutions. This front exhibits a number of different characteristics. A number of benchmarks should be created to expose the learning algorithms to all possible characteristics of the Pareto front.

\item Characteristics of a reward structure, such as different combinations of intrinsic and extrinsic rewards.
\end{itemize}

Vamplew et al (2011)\nocite{vamplew2011empirical} already introduced a number of benchmarks with known Pareto fronts. This greatly facilitates in the evaluation of the multi-objectives algorithms. We will use this these benchmarks as the basis for the empirical studies reported in this thesis.

Some of the benchmarks presented in Vamplew et al (2011)\nocite{vamplew2011empirical} are already being widely adopted in the MORL community. For example, Van Moffaert et al. (2012)\nocite{vamplew2011empirical}, Brys (2013)\nocite{brysmulti} etc.

\subsection{Deep sea treasure.}
\label{sec:deep-sea-treasure}
This test problem is a 10 by 11 grid world as shown in Figure \ref{fig:DSTOverview}. The grid represents an undersea surface with multiple number of treasure spots available. Each treasure spot has a different treasure value. The agent is represented as a submarine. Four actions are available to the agent - left, right, up, down. Each of the actions move the submarine by one square in appropriate direction; If an action results in the agent moving outside of the grid world then the agent is returned to his previous location. The reward signal for this problem is a two dimensional vector; the first component represents the amount of treasure that was found during a single episode, while the second component of the reward signal is an accumulated penalty over an episode (-1 for each move).

\begin{figure}[ht]
\vskip 0.2in
\centering
\includegraphics[scale=0.9]{dst.png}
\caption{Grid world for Deep Sea Treasure problem. Vamplew et al (2011).}
\label{fig:DSTOverview}
\end{figure}

The Deep Sea Treasure is an episodic test problem. Each episode the agent starts in the top-left corner of the gird world. During the exploration of the world the agent may find one of the treasure spots. Also, every time step the agent receives -1 penalty (to encourage shortest path learning); The episode finishes when the agent locates one of the treasures. After that the agent is returned to the starting state.

Also this test problem comes with a known Pareto front which is shown in Figure \ref{fig:DSTFront}. The form of the front is globally concave with number of local concavities.

\begin{figure}[ht]
\vskip 0.2in
\centering
\includegraphics[scale=0.9]{dstPareto.png}
\caption{Pareto front for Deep Sea Treasure problem. Vamplew et al (2011).}
\label{fig:DSTFront}
\end{figure}

\subsection{MO-Puddleworld.}
This test problem is a two dimensional grid world with puddles located across the world as shown on Figure \ref{fig:PWOverview}. Each episode the agent starts at a random location and must reach the goal position(top-right corner) while avoiding the puddles. Four actions are available to the agent - left, right, up, down. In this test problem the reward signal is represented as a two dimensional vector; The first component is an accumulated time step penalty over an episode (-1 for each move), while the second component of the reward vector is an accumulated puddle penalty (each time the agent walks into a puddle a penalty is given to the agent).

\begin{figure}[ht]
\vskip 0.2in
\centering
\includegraphics[scale=0.9]{pw.png}
\caption{Grid world for the Puddleworld problem. Vamplew et al (2011).}
\label{fig:PWOverview}
\end{figure}

MO-Puddleworld is also an episodic problem. Each episode the agent starts in a random or fixed location and must find its way to the goal state; Once the agent reaches the goal state, the episode is finished and the agent is returned to the start state.

MO-Puddleworld is a modification of the original Puddleworld problem (Boyan and Moore, 1995\nocite{boyan1995generalization}). Although several modifications were introduced:

\begin{itemize}
  \item One composite reward signal of the original Puddleworld was broken into a two component vector as described above.
  \item Calculation of the puddle penalty is slightly changed - the 400 multiplication is omitted.
  \item Original problem had a noise movement which was added during every move of the agent. It was ommited in the MO-Puddleworld.
  \item The goal state in the original problem was a triangular shape but it was enlarged to a full 0.05 square in the MO-Puddleworld.
\end{itemize}

\subsection{MO-Mountain Car.}
This test problem represents a valley with a car that must escape from the valley. But the car's engine is too weak to overcome the gravity force so the car must first climb a hill on the left side of the valley with reverse acceleration to build enough momentum to escape from the valley through the hill on the right side. Three actions are available to the agent - full throttle forward, full throttle backward, and zero throttle. A penalty of -1 is received on all states except the goal state. There are two additional penalties: -1 for each reverse acceleration and -1 for each forward acceleration.

As well as the MO-Puddleworld, the MO-Mountain Car is also a modification of the original single objective problem (Sutton, 1996\nocite{sutton1996generalization}). The original problem was modified by extending the reward signal from a scalar representation to a three component vector as described above.

\subsection{Resource gathering.}
\label{sec:resource-gathering}
This test problem, originally proposed by Barrett and Narayanan (2008)\nocite{barrett2008learning}, represents a 2-dimensional grid world. Two types of resources are available for gathering - gold and gems. The goal of the agent is to gather resources (either one of them or both). Also there are locations where the hero can be attacked, the chance of being attacked is 10 percent, see Figure \ref{fig:RGOverview} for details. If the attack happens then the agent loses all gathered resources and receives a negative reward for the "enemy" objective; The agent is also returned to the base after the attack. Four actions are available to the agent - left, right, up, down. Each of the actions move the agent by one square in appropriate direction.

The reward signal is a 3-dimensional vector where the first component shows the penalty if the enemy attack has happened, the second component shows how much gold was collected during an episode and the third component shows how much gems were collected.

\begin{figure}[ht]
\vskip 0.2in
\centering
\includegraphics[scale=0.6]{rg.png}
\caption{Grid world for the Resource Gathering problem. Barrett and Narayanan (2008).}
\label{fig:RGOverview}
\end{figure}

Barrett and Narayanan (2008)\nocite{barrett2008learning} used Convex Hull Value Iteration algorithm and found six optimal policies as shown on Figure \ref{fig:RGAlloptimalPolicies}

\begin{figure}[ht]
\vskip 0.2in
\centering
\includegraphics[scale=0.8]{rgpareto.png}
\caption{Optimal Policies for the Resource Gathering problem. The left hand side picture shows all non-dominated policies in the problem's state-space, while the right hand side shows the same policies in the objective space. Barrett and Narayanan (2008).}
\label{fig:RGAlloptimalPolicies}
\end{figure} 