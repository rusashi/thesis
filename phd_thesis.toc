\contentsline {chapter}{Abstract}{ii}{section*.1}
\contentsline {chapter}{Statement of Authorship}{iii}{section*.2}
\contentsline {chapter}{Acknowledgments}{iv}{chapter*.3}
\contentsline {chapter}{List of Tables}{vii}{chapter*.5}
\contentsline {chapter}{List of Figures}{viii}{chapter*.6}
\contentsline {chapter}{List of Publications}{x}{section*.7}
\contentsline {chapter}{\numberline {1}Literature review.}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Elements of reinforcement learning}{1}{section.1.1}
\contentsline {subsection}{\numberline {1.1.1}The Problem.}{1}{subsection.1.1.1}
\contentsline {subsection}{\numberline {1.1.2}Agent and environment}{2}{subsection.1.1.2}
\contentsline {subsection}{\numberline {1.1.3}Agent's goals.}{4}{subsection.1.1.3}
\contentsline {subsection}{\numberline {1.1.4}Dynamic Programming influence.}{5}{subsection.1.1.4}
\contentsline {subsubsection}{Value Functions and Bellman Equation.}{5}{section*.10}
\contentsline {subsubsection}{Optimal Value Functions.}{7}{section*.11}
\contentsline {subsubsection}{Value Iteration.}{7}{section*.12}
\contentsline {subsubsection}{Policy Iteration.}{8}{section*.13}
\contentsline {subsection}{\numberline {1.1.5}Q-Learning.}{9}{subsection.1.1.5}
\contentsline {subsection}{\numberline {1.1.6}Off-policy and On-policy algorithms.}{10}{subsection.1.1.6}
\contentsline {subsection}{\numberline {1.1.7}S.A.R.S.A.}{12}{subsection.1.1.7}
\contentsline {section}{\numberline {1.2}Multi-objective Optimization.}{12}{section.1.2}
\contentsline {subsection}{\numberline {1.2.1}Multi-objective Problem.}{13}{subsection.1.2.1}
\contentsline {subsection}{\numberline {1.2.2}Tradeoffs.}{13}{subsection.1.2.2}
\contentsline {subsection}{\numberline {1.2.3}Dominance.}{13}{subsection.1.2.3}
\contentsline {subsection}{\numberline {1.2.4}The Pareto Front.}{14}{subsection.1.2.4}
\contentsline {subsection}{\numberline {1.2.5}Selecting a solution in the Pareto Front.}{14}{subsection.1.2.5}
\contentsline {subsubsection}{Linear-Weighted Averages.}{15}{section*.14}
\contentsline {subsubsection}{Lexicographical Ordering.}{16}{section*.15}
\contentsline {section}{\numberline {1.3}Multi-objective Reinforcement Learning Research.}{17}{section.1.3}
\contentsline {subsection}{\numberline {1.3.1}Linear Temporal Difference Learning.}{17}{subsection.1.3.1}
\contentsline {subsection}{\numberline {1.3.2}Non-linear Temporal Difference Learning.}{20}{subsection.1.3.2}
\contentsline {subsection}{\numberline {1.3.3}Simultaneous Learning of More Than One Policy.}{23}{subsection.1.3.3}
\contentsline {chapter}{Introduction}{1}{section*.9}
\contentsline {chapter}{\numberline {2}Empirical Evaluation.}{27}{chapter.2}
\contentsline {section}{\numberline {2.1}Introduction.}{27}{section.2.1}
\contentsline {section}{\numberline {2.2}The Details of the Empirical Evaluation Study.}{29}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Performance metrics.}{30}{subsection.2.2.1}
\contentsline {subsubsection}{Hypervolume metric.}{31}{section*.16}
\contentsline {subsection}{\numberline {2.2.2}Experiment Structure.}{32}{subsection.2.2.2}
\contentsline {subsubsection}{Single-Policy Algorithms.}{32}{section*.17}
\contentsline {subsection}{\numberline {2.2.3}Bechmarking Software.}{34}{subsection.2.2.3}
\contentsline {subsection}{\numberline {2.2.4}Evaluation of the algorithms}{37}{subsection.2.2.4}
\contentsline {subsection}{\numberline {2.2.5}Single policy algorithms}{37}{subsection.2.2.5}
\contentsline {subsection}{\numberline {2.2.6}Multi-policy algorithms.}{39}{subsection.2.2.6}
\contentsline {section}{\numberline {2.3}Benchmarks.}{40}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}Deep sea treasure.}{41}{subsection.2.3.1}
\contentsline {subsection}{\numberline {2.3.2}MO-Puddleworld.}{43}{subsection.2.3.2}
\contentsline {subsection}{\numberline {2.3.3}MO-Mountain Car.}{44}{subsection.2.3.3}
\contentsline {subsection}{\numberline {2.3.4}Resource gathering.}{44}{subsection.2.3.4}
\contentsline {chapter}{Conclusions}{46}{section*.18}
\contentsline {chapter}{\textbf {References}}{46}{section*.19}
