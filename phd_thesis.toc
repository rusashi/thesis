\contentsline {chapter}{Abstract}{ii}{section*.1}
\contentsline {chapter}{Statement of Authorship}{iii}{section*.2}
\contentsline {chapter}{Acknowledgments}{iv}{chapter*.3}
\contentsline {chapter}{List of Tables}{viii}{chapter*.5}
\contentsline {chapter}{List of Figures}{ix}{chapter*.6}
\contentsline {chapter}{List of Publications}{xii}{section*.7}
\contentsline {chapter}{\numberline {1}Literature review.}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Elements of reinforcement learning}{1}{section.1.1}
\contentsline {subsection}{\numberline {1.1.1}The Problem.}{1}{subsection.1.1.1}
\contentsline {subsection}{\numberline {1.1.2}Agent and environment}{2}{subsection.1.1.2}
\contentsline {subsection}{\numberline {1.1.3}Agent's goals.}{4}{subsection.1.1.3}
\contentsline {subsection}{\numberline {1.1.4}Dynamic Programming influence.}{5}{subsection.1.1.4}
\contentsline {subsubsection}{Value Functions and Bellman Equation.}{5}{section*.11}
\contentsline {subsubsection}{Optimal Value Functions.}{7}{section*.12}
\contentsline {subsubsection}{Value Iteration.}{7}{section*.13}
\contentsline {subsubsection}{Policy Iteration.}{8}{section*.14}
\contentsline {subsubsection}{Utility Functions.}{9}{section*.15}
\contentsline {subsection}{\numberline {1.1.5}Q-Learning.}{10}{subsection.1.1.5}
\contentsline {subsection}{\numberline {1.1.6}Exploration vs Exploitation.}{12}{subsection.1.1.6}
\contentsline {subsection}{\numberline {1.1.7}Off-policy and On-policy algorithms.}{13}{subsection.1.1.7}
\contentsline {subsection}{\numberline {1.1.8}SARSA.}{14}{subsection.1.1.8}
\contentsline {section}{\numberline {1.2}Multi-objective Optimization.}{14}{section.1.2}
\contentsline {subsection}{\numberline {1.2.1}Multi-objective Problem.}{14}{subsection.1.2.1}
\contentsline {subsection}{\numberline {1.2.2}Tradeoffs.}{15}{subsection.1.2.2}
\contentsline {subsection}{\numberline {1.2.3}Dominance.}{15}{subsection.1.2.3}
\contentsline {subsection}{\numberline {1.2.4}The Pareto Front.}{16}{subsection.1.2.4}
\contentsline {subsection}{\numberline {1.2.5}Selecting a Solution in The Pareto Front.}{16}{subsection.1.2.5}
\contentsline {subsubsection}{Linear-Weighted Averages.}{17}{section*.17}
\contentsline {subsubsection}{Lexicographical Ordering.}{18}{section*.18}
\contentsline {section}{\numberline {1.3}Multi-objective Reinforcement Learning Research.}{18}{section.1.3}
\contentsline {subsection}{\numberline {1.3.1}Multi-Objective Markov Decission Processes (MOMDP).}{19}{subsection.1.3.1}
\contentsline {subsection}{\numberline {1.3.2}Linear Temporal Difference Learning.}{20}{subsection.1.3.2}
\contentsline {subsection}{\numberline {1.3.3}Non-linear Temporal Difference Learning.}{24}{subsection.1.3.3}
\contentsline {subsection}{\numberline {1.3.4}Simultaneous Learning of More Than One Policy.}{27}{subsection.1.3.4}
\contentsline {chapter}{Introduction}{1}{section*.9}
\contentsline {chapter}{\numberline {2}Empirical Evaluation.}{31}{chapter.2}
\contentsline {section}{\numberline {2.1}Introduction.}{31}{section.2.1}
\contentsline {section}{\numberline {2.2}The Empirical Evaluation Methodology.}{33}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Performance metrics.}{34}{subsection.2.2.1}
\contentsline {subsubsection}{Hypervolume metric.}{35}{section*.19}
\contentsline {subsection}{\numberline {2.2.2}Experiment Structure.}{37}{subsection.2.2.2}
\contentsline {subsubsection}{Single-Policy Algorithms.}{37}{section*.21}
\contentsline {subsubsection}{Multi-Policy Algorithms.}{40}{section*.25}
\contentsline {subsection}{\numberline {2.2.3}Benchmarking Software.}{41}{subsection.2.2.3}
\contentsline {subsection}{\numberline {2.2.4}Multi-Objective RL-Glue framework.}{44}{subsection.2.2.4}
\contentsline {section}{\numberline {2.3}Benchmarks.}{45}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}Deep sea treasure.}{46}{subsection.2.3.1}
\contentsline {subsection}{\numberline {2.3.2}MO-Puddleworld.}{47}{subsection.2.3.2}
\contentsline {subsection}{\numberline {2.3.3}MO-Mountain Car.}{49}{subsection.2.3.3}
\contentsline {subsection}{\numberline {2.3.4}Resource gathering.}{49}{subsection.2.3.4}
\contentsline {chapter}{\numberline {3}Evaluation of single policy multiobjective reinforcement learning algorithms.}{51}{chapter.3}
\contentsline {section}{\numberline {3.1}Introduction.}{51}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}Weighted Scalarization.}{52}{subsection.3.1.1}
\contentsline {subsection}{\numberline {3.1.2}Thresholded Lexicographic Ordering.}{52}{subsection.3.1.2}
\contentsline {section}{\numberline {3.2}Empirical Results.}{54}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Deep Sea Treasure.}{54}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}MO-PuddleWorld.}{54}{subsection.3.2.2}
\contentsline {subsection}{\numberline {3.2.3}MO-MountainCar.}{56}{subsection.3.2.3}
\contentsline {section}{\numberline {3.3}Conclusion.}{58}{section.3.3}
\contentsline {chapter}{Conclusions}{59}{section*.40}
\contentsline {chapter}{\textbf {References}}{59}{section*.41}
