\chapter{Evaluation of the single policy algorithms.}

\section{Introduction.}
Last two chapters provided necessary background information, as well as, arguments for empirical evaluation of multi-objective reinforcement learning algorithms. This and all following chapters will be concentrating on empirical analysis of different groups of algorithms. This chapter will focus on single policy algorithms. That is, algorithms which upon completion will produce an optimal value function for one policy. Arguably, two of the most important approaches are linear scalarization (see Section \ref{sec:linear-temporal-difference-learning}) and lexicographic ordering (see Section \ref{sec:non-linear-temporal-difference-learning}).

\subsection{Linear Scalarizaiton.}
An algorithm, which implements linear scalarization (for example Castelleti et al., 2002\nocite{castelletti2002reinforcement}), can locate a single point of the Pareto front. The located point depends on a set of preferences which was given to the algorithm. By gradually changing the preferences over objectives the algorithm can locate optimal value functions for different policies, thus approximating the Pareto front.

An experiment structure for a single policy algorithm based on the linear scalarization was given in Section \ref{sec:experiment-structure}. It is important to point out that a priori an experimenter does not know which preferences correspond to which policy and the experimenter will need to perform a number of test learning episodes to get an idea of a shape of a Pareto front for some MOMDP.

\subsection{TLO.}
TLO, due to its nature, requires some knowledge about the problem under consideration. Namely, thresholds should be provided for the first n-1 objectives, which clearly require some prior knowledge of the range of values expected for each objective. One way to obtain the required knowledge is to observe the results produced by the WS algorithm. The WS produced approximation to the Pareto front and by observing the actual points from that approximated front we can identify the region of objective space in which all points fell. Thus we can find min and max value for first n-1 objectives. Then we can use that information to tell TLO to look in the specific objective space region. Effectively we bound first n-1 objectives and leave the last objective unbound to see whether TLO can produce better results than WS. The following pseudo code sketches an action selection mechanism for the TLO algorithm.

\section{Benchmarks.}

Provide empirical results of all algorithms.

\section{Limitations.}

Give interpretation of the empirical results and describe strengths an weaknesses of each of the algorithms.
